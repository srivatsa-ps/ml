{"cells":[{"cell_type":"markdown","metadata":{"id":"EI-0b9VE3ymp"},"source":["Exercise 3: Support Vector Machines and K Nearest Neighbours Implementation"]},{"cell_type":"markdown","metadata":{"id":"Ek1naJul3ymq"},"source":["****SUPPORT VECTOR MACHINES****"]},{"cell_type":"markdown","metadata":{"id":"fLTcQeGY3ymq"},"source":["**Step 1: Import all necessary modules**"]},{"cell_type":"markdown","metadata":{"id":"m2xyOWGQ3ymr"},"source":["The line from sklearn.svm import SVC imports the SVC (Support Vector Classifier) class from the sklearn.svm module. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYeJt_nD3ymr"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"btgMNoc53ymr"},"source":["**Step 2: Load the Iris Dataset from the datasets provided by sklearn**\n","\n"," X contains all the rows of the first two features ans we then split it into training set and testing set."]},{"cell_type":"markdown","metadata":{"id":"Nnbobnxs3yms"},"source":["The first two lines of code load the iris dataset from scikit-learn's built-in datasets module. The iris dataset contains 150 samples of iris flowers, with 50 samples for each of three different species (Setosa, Versicolour, and Virginica). Each sample has four features (sepal length, sepal width, petal length, and petal width) and a target variable indicating the species of the iris.\n","\n","The third and fourth lines of code extract the first two features (sepal length and sepal width) from the dataset as X, and the target variable (iris species) as y."]},{"cell_type":"markdown","metadata":{"id":"c60fPlYX3yms"},"source":[" iris.data[:, :2] is used to select all rows of the dataset and the first two columns. This syntax selects the first two features (columns) of the iris dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ZknqIzT3yms"},"outputs":[],"source":["iris = datasets.load_iris()\n","X = iris.data[:, :2]\n","y = iris.target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaZ5YF7q3yms"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"1GiYqCzi3ymt"},"source":["**Step 3: Create an SVM Classifier with Linear, Radial Base and Polykernel Kernel Function**\n","\n","Make predictions on the test data and calculate the accuracy."]},{"cell_type":"markdown","metadata":{"id":"eqh4y0Wh3ymt"},"source":["In this code, we are instantiating a Support Vector Machine (SVM) classifier using the SVC() function from scikit-learn's svm module.\n","\n","The kernel parameter specifies the type of kernel to be used. In this case, we are using a linear kernel.\n","\n","The C parameter controls the trade-off between achieving a low training error and a low testing error that is necessary to avoid overfitting.\n","\n","Smaller values of C lead to wider margins but more errors on the training data, while larger C values lead to narrower margins but fewer errors on the training data.\n","\n","the margins are the maximum width separating the decision boundary and the closest points from each class. In other words, they represent the largest \"street\" possible between the decision boundary and the closest points from each class. The points closest to the boundary are called support vectors, and they determine the position of the boundary. Maximizing the margin between the classes helps improve the generalization performance of the model, as it minimizes the probability of misclassification of new, unseen data.\n","A smaller value of C creates a wider margin and leads to a larger number of misclassified points but better generalization. A larger value of C leads to a narrower margin and fewer misclassified points but potentially worse generalization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbB-6vEv3ymt","outputId":"7dfe33df-722d-4aab-b1b6-0afde8d02a3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.80\n"]}],"source":["clf = SVC(kernel='linear', C=1.0, random_state=0)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"i7zXprRT3ymu"},"source":["In this code, we have instantiated an instance of the SVC class with the RBF (Radial basis function) kernel. The RBF kernel is used to handle non-linearly separable data.\n","\n","gamma parameter ??"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BhCG_hu3ymu","outputId":"7e06b3bf-daf3-4519-faf2-d9a43bc89730"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.80\n"]}],"source":["C = 1.0 # regularization parameter\n","gamma = 0.1 # kernel coefficient for RBF kernel\n","clf = SVC(kernel='rbf', C=C, gamma=gamma)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"GlM2WvT43ymu"},"source":["The degree parameter determines the degree of the polynomial kernel function. Increasing the degree can lead to more complex decision boundaries that can better fit the training data, but also increases the risk of overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3POeW1023ymu","outputId":"6f9cc40c-3d8b-46a4-ca22-7ab7e5ffc6b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.80\n"]}],"source":["C = 1.0 # regularization parameter\n","degree = 3 # degree of the polynomial kernel function\n","clf = SVC(kernel='poly', C=C, degree=degree)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"ugio1rol3ymu"},"source":["****K NEAREST NEIGHBOURS****"]},{"cell_type":"markdown","metadata":{"id":"dztoG6d73ymu"},"source":["**Step 1: Import all necessary modules**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JV9jJwrU3ymv"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"Rt0CZg-I3ymv"},"source":["**Step 2: Load the Iris Dataset from the datasets provided by sklearn**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fA12t3WX3ymv"},"outputs":[],"source":["iris = load_iris()\n","X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=1)\n"]},{"cell_type":"markdown","metadata":{"id":"EczIGZ2o3ymv"},"source":["**Step 3: Create an KNN Classifier**\n","\n","Make predictions on the test data and calculate the accuracy."]},{"cell_type":"markdown","metadata":{"id":"hfkux9nN3ymv"},"source":["The code above is using the k-nearest neighbors classifier from the scikit-learn library to fit a model to the training data and make predictions on the test data.\n","\n","The parameter n_neighbors specifies the number of neighbors to consider when classifying a new data point. In this case, k is set to 5, meaning that the classifier will consider the 5 closest neighbors to each test data point."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KiJEGmSL3ymv","outputId":"ba45a2cb-540e-4819-acda-d02b18531870"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.98\n"]}],"source":["k = 5\n","clf = KNeighborsClassifier(n_neighbors=k)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print('Accuracy: %.2f' % accuracy)"]}],"metadata":{"kernelspec":{"display_name":".env","language":"python","name":".env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}